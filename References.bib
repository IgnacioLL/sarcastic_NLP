@book{geron2019hands,
  title={Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems},
  author={G{\'e}ron, Aur{\'e}lien},
  year={2019},
  publisher={" O'Reilly Media, Inc."}
}

@misc{LSTM_cite,
  title = {The Unreasonable Effectiveness of Recurrent Neural Networks},
  howpublished = {\url{http://karpathy.github.io/2015/05/21/rnn-effectiveness/}},
  year = {2015}, 
  author = {Andrej Karparthy}
}

@misc{LSA,
  title = {Global Vectors for Word Representation},
  howpublished = {\url{https://nlp.stanford.edu/pubs/glove.pdf}},
  year = {2014}, 
  author = {Jeffrey Pennington et. Al}
}

@misc{visualization_word_embedd,
  title = {Visualizing feature vectors/embeddings using t-SNE and PCA},
  howpublished = {\url{https://nlp.stanford.edu/pubs/glove.pdf}},
  year = {2021}, 
  author= {Manpreet Singh Minhas}
}

@misc{MaxPool,
  doi = {10.48550/ARXIV.1908.05040},
  
  howpublished = {\url{https://arxiv.org/abs/1908.05040}},
  
  author = {Christlein, Vincent and Spranger, Lukas and Seuret, Mathias and Nicolaou, Anguelos and Kr√°l, Pavel and Maier, Andreas},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Generalized Max Pooling},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{LSTM_explanation,
 
  howpublished = {\url{https://www.youtube.com/watch?v=8HyCNIVRbSU}},
  
  author = {Michael Phi},

  title = {Illustrated Guide to LSTM's and GRU's: A step by step explanation},
  
  year = {2018},
  }


@misc{Attention,
  doi = {10.48550/ARXIV.1706.03762},
  
  howpublished = {\url{https://arxiv.org/abs/1706.03762}},
  
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{BERT,
  doi = {10.48550/ARXIV.1810.04805},
  
  howpublished = {\url{https://arxiv.org/abs/1810.04805}},
  
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{distilBERT,
  doi = {10.48550/ARXIV.1910.01108},
  
  howpublished = {\url{https://arxiv.org/abs/1910.01108}},
  
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{evolved_transformer,
  doi = {10.48550/ARXIV.1901.11117},
  
  howpublished = {\url{https://arxiv.org/abs/1901.11117}},
  
  author = {So, David R. and Liang, Chen and Le, Quoc V.},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Evolved Transformer},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{arquitecturas,
	doi = {10.1109/access.2021.3077350},
  
	howpublished = {https://arxiv.org/abs/2104.10640},
  
	year = 2021,
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {9},
  
	pages = {68675--68702},
  
	author = {Sushant Singh and Ausif Mahmood},
  
	title = {The {NLP} Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures},
  
	journal = {{IEEE} Access}
}


@misc{embeedings,
  title = {Neural Network Embeddings Explained},
  howpublished = {\url{https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526}},
  year = {2018}, 
  author= {Will Koehrsen}
}

@misc{transformers,
  title = {NLP for Developers: Transformers | Rasa},
  howpublished = {\url{https://www.youtube.com/watch?v=KN3ZL65Dze0}},
  year = {2020}, 
  author= {Rasa}
}



@misc{dropout,
  title = {Dropout Explained},
  howpublished = {\url{https://leimao.github.io/blog/Dropout-Explained/}},
  year = {2019}, 
  author= {Lei Mao}
}

@misc{perceptron_train,
  title = {Perceptron Training},
  howpublished = {\url{https://youtu.be/5g0TPrxKK6o}},
  year = {2015}, 
  author= {Udacity}
}

@article{marvin1969perceptrons,
  title={Perceptrons},
  author={Marvin, Minsky and Seymour, A Papert},
  journal={Cambridge, MA: MIT Press},
  volume={6},
  pages={318--362},
  year={1969}
}