# NLP Benchmark
### Introduction
This repository contains a benchmark of 3 NLP models: LSTM, BERT and DISTILBERT for predicting sarcasm in sentences. The dataset used for training and testing the models was extracted from Kaggle and consists of sentences labeled as sarcastic or not. The benchmark was conducted as part of a final degree project in python using TensorFlow.

#### Requirements
- Python 3
- TensorFlow
- pandas
- numpy

### Results
The results of the benchmark, including accuracy and loss for each model, can be found in the memoria file. The final conclusion of the benchmark is discussed in the accompanying article, written in LaTeX.

### References
- Kaggle dataset: Sarcasm in Reddit
### Note
- The models used in this benchmark are not intended for commercial use and are for educational purposes only.
